{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pyautogen\n",
    "#%pip install pyarrow\n",
    "#%pip install scikit-learn matplotlib seaborn IPython tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load MIMIC III or MIMIC IV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "MIMIC_DB_LOCATION = '../../MIMIC-III/1.4/'\n",
    "#MIMIC_DB_LOCATION = '../../MIMIC-IV'\n",
    "\n",
    "MEDICAL_CONDITION_NAME = \"Acute Kidney Injury\"\n",
    "MEDICAL_CONDITION_ICD_KEYWORDS = [\"acute kidney injury\", \"acute kidney failure\", \"aki\"]\n",
    "\n",
    "# Create the coding folder if it does not exist\n",
    "CODING_FOLDER = \"autogen\"\n",
    "if not os.path.exists(CODING_FOLDER):\n",
    "    os.makedirs(CODING_FOLDER)\n",
    "\n",
    "# Create the data folder if it does not exist\n",
    "DATA_FOLDER = \"data\"\n",
    "if not os.path.exists(f\"{CODING_FOLDER}/{DATA_FOLDER}\"):\n",
    "    os.makedirs(f\"{CODING_FOLDER}/{DATA_FOLDER}\")\n",
    "    \n",
    "# Create an array of tables to load\n",
    "tables = ['patients', 'admissions', 'diagnoses_icd', 'd_icd_diagnoses', 'labevents', 'd_labitems']\n",
    "\n",
    "# Create a dict for all the dataframes\n",
    "df = {}\n",
    "\n",
    "for table in tables:\n",
    "    # Load the the table if it exists\n",
    "    if os.path.exists(f'{MIMIC_DB_LOCATION}/{table}.parquet'):\n",
    "        df[table] = pd.read_parquet(f'{MIMIC_DB_LOCATION}/{table}.parquet')\n",
    "        continue\n",
    "    elif os.path.exists(f'{MIMIC_DB_LOCATION}/{table}.csv.gz'):\n",
    "        df[table] = pd.read_csv(f'{MIMIC_DB_LOCATION}/{table}.csv.gz', compression='gzip')\n",
    "    elif os.path.exists(f'{MIMIC_DB_LOCATION}/{table}.csv'):\n",
    "        df[table] = pd.read_csv(f'{MIMIC_DB_LOCATION}/{table}.csv')\n",
    "    elif os.path.exists(f'{MIMIC_DB_LOCATION}/{table.upper()}.csv.gz'):\n",
    "        df[table] = pd.read_csv(f'{MIMIC_DB_LOCATION}/{table.upper()}.csv.gz', compression='gzip')\n",
    "    elif os.path.exists(f'{MIMIC_DB_LOCATION}/{table.upper()}.csv'):\n",
    "        df[table] = pd.read_csv(f'{MIMIC_DB_LOCATION}/{table.upper()}.csv')\n",
    "    else:\n",
    "        print(f'Could not find {table} in {MIMIC_DB_LOCATION}')\n",
    "        continue\n",
    "\n",
    "    # convert all columns to lowercase\n",
    "    df[table].columns = df[table].columns.str.lower()\n",
    "    \n",
    "    # Save the dataframe to parquet\n",
    "    df[table].to_parquet(f'{MIMIC_DB_LOCATION}/{table}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing patient data and convert datetime columns\n",
    "if 'anchor_age' in df['patients'].columns: # MIMIC-IV\n",
    "    df['patients'].dropna(subset=['gender', 'anchor_age', 'anchor_year'], inplace=True)\n",
    "    df['patients']['anchor_year_datetime'] = pd.to_datetime(df['patients']['anchor_year'].astype(str) + '-01-01')  \n",
    "else: # MIMIC-III\n",
    "    df['patients'].dropna(subset=['gender', 'dob'], inplace=True)\n",
    "    df['patients'].dob = pd.to_datetime(df['patients'].dob)\n",
    "\n",
    "# change gender to 0 and 1 using a lambda\n",
    "df['patients']['gender'] = df['patients']['gender'].apply(lambda x: 0 if x == 'F' else 1)\n",
    "df['patients']['gender'] = df['patients']['gender'].astype(int)\n",
    "\n",
    "# Drop missing admission data\n",
    "df['admissions'].dropna(subset=['subject_id', 'hadm_id', 'admittime', 'dischtime'], inplace=True)\n",
    "\n",
    "# Convert admission and discharge times to datetime\n",
    "df['admissions'].admittime = pd.to_datetime(df['admissions'].admittime)\n",
    "df['admissions'].dischtime = pd.to_datetime(df['admissions'].dischtime)\n",
    "\n",
    "# Remove admissions where admission time is after discharge time\n",
    "df['admissions'] = df['admissions'][df['admissions'].admittime < df['admissions'].dischtime]\n",
    "\n",
    "# Convert the charttime to datetime\n",
    "df['labevents'][\"charttime\"] = pd.to_datetime(df['labevents'][\"charttime\"])\n",
    "\n",
    "# Drop any rows where hadm_id, valuenum is missing\n",
    "df['labevents'] = df['labevents'].dropna(subset=['hadm_id', 'valuenum'])\n",
    "\n",
    "# Filter out any lab events that are not within the admission time\n",
    "df['labevents'] = df['labevents'].merge(df['admissions'][['hadm_id', 'admittime', 'dischtime']], on='hadm_id')\n",
    "df['labevents'] = df['labevents'][(df['labevents'].charttime >= df['labevents'].admittime) & \n",
    "                            (df['labevents'].charttime <= df['labevents'].dischtime)]\n",
    "\n",
    "# Clean up the lab items data\n",
    "df['d_labitems'] = df['d_labitems'].dropna(subset=['itemid', 'label'])\n",
    "\n",
    "# Clean up diagnoses_icd and d_icd_diagnoses\n",
    "if 'icd9_code' in df['diagnoses_icd'].columns:\n",
    "    df['diagnoses_icd'].rename(columns={'icd9_code': 'icd_code'}, inplace=True)\n",
    "    df['d_icd_diagnoses'].rename(columns={'icd9_code': 'icd_code'}, inplace=True)\n",
    "\n",
    "df['diagnoses_icd'] = df['diagnoses_icd'].dropna(subset=['subject_id', 'hadm_id', 'icd_code'])\n",
    "df['d_icd_diagnoses'] = df['d_icd_diagnoses'].dropna(subset=['icd_code', 'long_title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate age at time of admission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate age function\n",
    "def calculate_age(date1, date2):\n",
    "    date1 = date1.to_pydatetime()\n",
    "    date2 = date2.to_pydatetime()\n",
    "    age = (date2 - date1).days // 365.25\n",
    "\n",
    "    return age\n",
    "\n",
    "# Perform an inner join between admissions_df and patients_df on subject_id  \n",
    "patients_admissions_df = pd.merge(df['admissions'], df['patients'], on='subject_id')  \n",
    "\n",
    "if 'anchor_age' in df['patients'].columns: # MIMIC-IV\n",
    "    patients_admissions_df['age_at_admission'] = patients_admissions_df['anchor_age'] + (patients_admissions_df['admittime'].dt.year - patients_admissions_df['anchor_year_datetime'].dt.year) \n",
    "else: # MIMIC-III\n",
    "    patients_admissions_df[\"age_at_admission\"] = patients_admissions_df.apply(lambda row: calculate_age(row[\"dob\"], row[\"admittime\"]), axis=1)\n",
    "\n",
    "# Rename age_at_admission column to age\n",
    "patients_admissions_df.rename(columns={'age_at_admission': 'age'}, inplace=True)\n",
    "\n",
    "# Filter out any patients that are less than 18 years old and older than 90 years old at the time of admission\n",
    "patients_admissions_df = patients_admissions_df[(patients_admissions_df.age >= 18) & (patients_admissions_df.age <= 90)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter lab events for chemitry based labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter labevents_df based on itemid and valuenum conditions  \n",
    "patients_labs_df = df['labevents'][\n",
    "    (df['labevents']['itemid'].isin([50862, 50930, 50976, 50868, 50882, 50893, 50912, 50902, 50931, 50983, 50971, 51006])) &\n",
    "    ((df['labevents']['valuenum'] > 0) | (df['labevents']['itemid'] == 50868)) &\n",
    "    (df['labevents']['valuenum'].notnull())\n",
    "].copy()\n",
    "\n",
    "# Apply conditional logic to create columns based on itemid and valuenum conditions  \n",
    "conditions = {  \n",
    "    'albumin': (patients_labs_df['itemid'] == 50862) & (patients_labs_df['valuenum'] <= 10),  \n",
    "    'globulin': (patients_labs_df['itemid'] == 50930) & (patients_labs_df['valuenum'] <= 10),  \n",
    "    'total_protein': (patients_labs_df['itemid'] == 50976) & (patients_labs_df['valuenum'] <= 20),  \n",
    "    'aniongap': (patients_labs_df['itemid'] == 50868) & (patients_labs_df['valuenum'] <= 10000),  \n",
    "    'bicarbonate': (patients_labs_df['itemid'] == 50882) & (patients_labs_df['valuenum'] <= 10000),  \n",
    "    'bun': (patients_labs_df['itemid'] == 51006) & (patients_labs_df['valuenum'] <= 300),  \n",
    "    'calcium': (patients_labs_df['itemid'] == 50893) & (patients_labs_df['valuenum'] <= 10000),  \n",
    "    'chloride': (patients_labs_df['itemid'] == 50902) & (patients_labs_df['valuenum'] <= 10000),  \n",
    "    'creatinine': (patients_labs_df['itemid'] == 50912) & (patients_labs_df['valuenum'] <= 150),  \n",
    "    'glucose': (patients_labs_df['itemid'] == 50931) & (patients_labs_df['valuenum'] <= 10000),  \n",
    "    'sodium': (patients_labs_df['itemid'] == 50983) & (patients_labs_df['valuenum'] <= 200),  \n",
    "    'potassium': (patients_labs_df['itemid'] == 50971) & (patients_labs_df['valuenum'] <= 30)  \n",
    "}  \n",
    "  \n",
    "for col, cond in conditions.items():  \n",
    "    patients_labs_df[col] = np.where(cond, patients_labs_df['valuenum'], np.nan)  \n",
    "\n",
    "patients_labs_df = patients_labs_df.groupby(['hadm_id', 'charttime']).agg({\n",
    "    'albumin': 'max',  \n",
    "    'globulin': 'max',  \n",
    "    'total_protein': 'max',  \n",
    "    'aniongap': 'max',  \n",
    "    'bicarbonate': 'max',  \n",
    "    'bun': 'max',  \n",
    "    'calcium': 'max',  \n",
    "    'chloride': 'max',  \n",
    "    'creatinine': 'max',  \n",
    "    'glucose': 'max',  \n",
    "    'sodium': 'max',  \n",
    "    'potassium': 'max'    \n",
    "}).reset_index()\n",
    "\n",
    "# Sort by hadm_id and charttime\n",
    "patients_labs_df = patients_labs_df.sort_values(by=['hadm_id', 'charttime'])\n",
    "\n",
    "# Drop any rows where all values are missing (except for hadm_id and charttime)\n",
    "patients_labs_df = patients_labs_df.dropna(subset=patients_labs_df.columns[2:], how='all')\n",
    "\n",
    "# Drop any rows from labs_df where hadm_id is not in patients_df\n",
    "patients_labs_df = patients_labs_df[patients_labs_df['hadm_id'].isin(patients_admissions_df['hadm_id'])]\n",
    "\n",
    "# Drop any rows from patients_df where hadm_id is not in labs_df\n",
    "patients_info_df = patients_admissions_df[patients_admissions_df['hadm_id'].isin(patients_labs_df['hadm_id'])][['hadm_id', 'age', 'gender']]\n",
    "\n",
    "# Save labs_df to data folder\n",
    "patients_labs_df.to_parquet(f\"{CODING_FOLDER}/{DATA_FOLDER}/patients_labs.parquet\")\n",
    "\n",
    "# Save patients_admissions_df to data folder\n",
    "patients_info_df.to_parquet(f\"{CODING_FOLDER}/{DATA_FOLDER}/patients_info.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a filtered list of ICD9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df['icd_diagnoses'] based on hadm in patients_labs_df\n",
    "filtered_icd_diagnoses_df = df['diagnoses_icd'][df['diagnoses_icd']['hadm_id'].isin(patients_labs_df['hadm_id'])]\n",
    "\n",
    "# Convert icd_code column to lowercase using .loc\n",
    "filtered_icd_diagnoses_df.loc[:, 'icd_code'] = filtered_icd_diagnoses_df['icd_code'].str.lower()\n",
    "\n",
    "# Save to data folder\n",
    "filtered_icd_diagnoses_df[['hadm_id', 'icd_code']].to_parquet(f\"{CODING_FOLDER}/{DATA_FOLDER}/patients_diagnoses.parquet\")\n",
    "\n",
    "# Filter d_icd_diagnoses based on icd_code in filtered_icd_diagnoses_df\n",
    "filtered_d_icd_diagnoses_df = df['d_icd_diagnoses'][df['d_icd_diagnoses']['icd_code'].isin(filtered_icd_diagnoses_df['icd_code'])]\n",
    "\n",
    "# Save to data folder\n",
    "filtered_d_icd_diagnoses_df[['icd_code', 'long_title']].to_parquet(f\"{CODING_FOLDER}/{DATA_FOLDER}/icd_codes.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task list for AutoGen to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of columns in patients_labs_df and remove hadm_id and charttime\n",
    "lab_test_types = list(patients_labs_df.columns)\n",
    "\n",
    "# Drop hadm_id and charttime from lab_test_types\n",
    "lab_test_types.remove('hadm_id')\n",
    "lab_test_types.remove('charttime')\n",
    "\n",
    "tasks = [\n",
    "# Research    \n",
    "    f\"\"\"You are a Healthcare Specialist. Given the medical condition, {MEDICAL_CONDITION_NAME}, what are the key indicators and criteria based on blood chemisty lab tests that can be use to predict the onset of the medical condition. \n",
    "    - Please don't use any web scraping or external data sources.\n",
    "    - Only include the chemistry lab tests types that are in the following list (lab_test_types):\n",
    "\n",
    "{lab_test_types} \n",
    "\n",
    "Using a Python code block (research.py) Save your findings to '{DATA_FOLDER}/lab_test_types.json' as an array of lab test types.\n",
    "    \"\"\",\n",
    "\n",
    "# Processing / Filtering\n",
    "    f\"\"\"You are a Data Scientist with Python development skills.  Please generate the code to perform the following tasks in the same Python code block (named processing_filtering.py):\n",
    "1. Load '{DATA_FOLDER}/patients_labs.parquet' into pandas dataframe (labs).\n",
    "2. Load '{DATA_FOLDER}/lab_test_types.json' and create a list of lab test types (lab_test_types).\n",
    "4. Remove any values in the lab_test_types list that do not exist in the columns of labs dataframe.\n",
    "5. Remove any columns (except hadm_id, charttime) in the labs dataframe that do not exist in the list of lab_test_types.\n",
    "6. Remove any rows where all the lab_test_types columns are null.\n",
    "7. Save the labs dataframe to  '{DATA_FOLDER}/filtered_patients_labs.parquet'.\n",
    "    \"\"\",\n",
    "\n",
    "# Labeling\n",
    "    f\"\"\"You are a Data Scientist with Python development skills. Please generate the code to perform the following tasks in the same Python code block (named labeling.py):\n",
    "1. Load the following parquet files in to pandas dataframes\n",
    "2. Load '{DATA_FOLDER}/patients_diagnoses.parquet' into pandas dataframe (diagnoses).\n",
    "3. Load '{DATA_FOLDER}/icd_codes.parquet' into pandas dataframe (icd_codes).\n",
    "4. Create a list of icd_codes (condition_codes) where the long_title column contains (case insensitive) any of the following keywords: {MEDICAL_CONDITION_ICD_KEYWORDS}\n",
    "5. Create a unique list of hadm_ids (positive_diagnoses) from diagnoses dataframe where the icd_code is in the condition_codes list.\n",
    "6. Create a new dataframe (labels) with the following columns:\n",
    "    - hadm_id (unique from labs dataframe)\n",
    "    - condition_label (1 if hadm_id is in positive_diagnoses list, 0 otherwise)\n",
    "7. Save the labels as \"{DATA_FOLDER}/patients_labels.parquet\".\n",
    "    \"\"\",\n",
    "\n",
    "# Feature Engineering\n",
    "    f\"\"\"You are a Data Scientist with Python development skills who specializes in feature engineering for machine learning models. \n",
    "Please generate the code to perform the following tasks in the same Python code block (named feature_engineering.py):\n",
    "1. Load the following parquet files into pandas dataframes\n",
    "    - '{DATA_FOLDER}/filtered_patients_labs.parquet' into pandas dataframe (labs).\n",
    "    - '{DATA_FOLDER}/patients_info.parquet' into pandas dataframe (patient_info).\n",
    "    - '{DATA_FOLDER}/patients_labels.parquet' into pandas dataframe (labels).    \n",
    "2. Generate a list of lab test columns (lab_tests) from the labs dataframe.\n",
    "    - Excluding hadm_id, charttime columns \n",
    "3. Group labs dataframe by hadm_id and charttime and take the mean for each column (grouped_labs).\n",
    "4. Sort the grouped_labs dataframe by hadm_id and charttime.\n",
    "5. For each column (lab_test) in grouped_labs that exists in lab_tests, calculate the following features:\n",
    "    - difference from baseline value (lab_test_baseline_delta)\n",
    "    - delta from previous value (lab_test_diff)\n",
    "    - time difference in hours from previous value (lab_test_timediff)\n",
    "    - rate of change per day (lab_test_rateofchange)\n",
    "6. Drop the following columns:\n",
    "    - charttime\n",
    "    - lab_test_timediff\n",
    "7. Grouped the dataframe by hadm_id and aggregate in the following way:\n",
    "    - Generate (mean, median, std, min, max) for the following engineered features for each lab_test column:\n",
    "        - lab_test\n",
    "        - lab_test_baselinedelta\n",
    "        - lab_test_delta\n",
    "        - lab_test_rateofchange\n",
    "8. Flatten the multi-index columns to a single level (engineered_features)\n",
    "9. Fix the the column names by removing trailing underscores.\n",
    "10. Impute engineered_features to fill any missing values using a simple imputer.\n",
    "11. Merge the patient_info dataframe with the engineered_features dataframe on hadm_id (features)\n",
    "13. Merge the labels dataframe with the features dataframe on hadm_id (features_labels).\n",
    "12. Drop any rows with missing values.\n",
    "15. Drop the hadm_id column from the features_labels dataframe.\n",
    "16. Save the features_labels as \"{DATA_FOLDER}/features_labels.parquet\".\n",
    "    \"\"\",\n",
    "\n",
    "# Dimensionality Reduction\n",
    "    f\"\"\"You are an AI Engineer with Python development skills that specializes in dimensionality reduction. Please generate the code to perform the following tasks in the same Python code block (named dimensionality_reduction.py):\n",
    "1. Load the following parquet files into pandas dataframes\n",
    "    - '{DATA_FOLDER}/features_labels.parquet' into pandas dataframe (features_labels).\n",
    "2. Split the features_labels dataframe into features and labels dataframes with the labels being the condition_label column.\n",
    "3. Perform dimensionality reduction on the features based on your recommended method for use with a classification model.\n",
    "4. Make sure the columns names of the reduced features are strings.\n",
    "5. Combine the reduced features and labels (reduced_features_labels).\n",
    "6. Save reduced_features_labels to a new parquet file: '{DATA_FOLDER}/reduced_features_labels.parquet'.\n",
    "7. Print the original number of features and number of features retained after dimensionality reduction.\n",
    "\n",
    "After the execution of the Python code, please provide a brief explanation of the dimensionality reduction method used, why it was chosen, and what features were retained (if possible).\n",
    "    \"\"\",\n",
    "\n",
    "# Model Training and Evaluation\n",
    "    f\"\"\"You are an AI Engineer with Python development skills. Please generate the code to perform the following tasks in the same Python code block (named training_evaluation.py):\n",
    "1. Load the follwing parquet file: '{DATA_FOLDER}/reduced_features_labels.parquet' into a pandas dataframe.\n",
    "    - This dataframe contains a set of features and one binary label (condition_label)\n",
    "2. Split the dataframe into features (X) and labels (y) dataframes with the labels being the condition_label column.\n",
    "3. Split the data into training (X_train, y_train) and testing sets (X_test, y_test).\n",
    "4. Train the following classifiers on the training data:\n",
    "    - DecisionTreeClassifier\n",
    "        - max_depth=5\n",
    "        - random_state=42\n",
    "    - RandomForestClassifier\n",
    "        - n_estimators=300 \n",
    "        - max_depth=None\n",
    "        - min_samples_split=2\n",
    "        - min_samples_leaf=2\n",
    "        - random_state=42\n",
    "        - n_jobs=-1\n",
    "    - LogisticRegression\n",
    "        - max_iter=1000\n",
    "        - n_jobs=-1\n",
    "        - random_state=42\n",
    "    - GradientBoostingClassifier\n",
    "        - n_estimators=300\n",
    "        - random_state=42\n",
    "    - MLPClassifier\n",
    "        - alpha=1, \n",
    "        - max_iter=1000\n",
    "        - random_state=42\n",
    "    - KNeighborsClassifier\n",
    "        - n_neighbors=5\n",
    "        - n_jobs=-1\n",
    "5. Evaluate each model on the testing data and perform tasks on it:\n",
    "    - Generate a classification report based on X_test and y_test and save it in a dict (classification_report)\n",
    "    - Calculate the roc curve (roc_curve) based on X_test and y_test and convert it to a dict with the columns (fpr, tpr, auc)\n",
    "    - Calculate the precision-recall curve (pr_curve) based on X_test and y_test and convert it to a dict with the columns (precision, recall, auc)\n",
    "    - Save the model as a pickle file to  '{DATA_FOLDER}/model_type.pkl'.\n",
    "5. Create a dict (model_details) with the model names as keys and the values as the dicts (classification_report, roc_curve, pr_curve) and model_path.\n",
    "6. Save the model_details as JSON to '{DATA_FOLDER}/model_details.json'.\n",
    "7. For each model in model_details load the classification report as a pandas dataframe and print it as a markdown table.\n",
    "\n",
    "After the execution of the Python code, please provide a detail analysis of each model by describing what the classification report metrics mean.\n",
    "\n",
    "Give your detailed analysis, please provide a brief explanation of the model that performed the best and why it was chosen.\n",
    "    \"\"\",\n",
    "\n",
    "# Model Visualization\n",
    "    f\"\"\"You are aa Visualization Expert with Python development skills. Please generate the code to perform the following tasks in the same Python code block (named visualization.py):\n",
    "1. Load the model performance details from '{DATA_FOLDER}/model_details.json' into a pandas dataframe.\n",
    "    - The keys in the JSON file are the model names and the columns (classification_report, roc_curve, pr_curve) are the model performance data.\n",
    "2. Based on the classification report data (classification_report), create a combined bar plot comparing the report data for each model.\n",
    "    - Create a dict (cr_data) with the model names as keys and the value: precision (from weighted avg), recall (from weighted avg), f1-score (from weighted avg), accuracy.\n",
    "    - Plot the Performance Metric (precision, recall, f1-score and accuracy) for each model based on cr_data grouped by the model names\n",
    "        - Group the bars by the model name with bars for each performance metric (precision, recall, f1-score and accuracy).\n",
    "        - Set the hue to the Performance Metric key.\n",
    "        - Scale the min of the y-axis to be slightly less than the min value of the data.\n",
    "        - Scale the max of the y-axis to be slightly more than the max value of the data.\n",
    "        - Remove \"Classifier\" from the model names for better readability.\n",
    "        - Rotate the x-axis labels for better readability.\n",
    "    - Save the plot to '{DATA_FOLDER}/classification_report.png'\n",
    "3. Based on the roc curve data (roc_curve), create a combined line plot of the roc curve for each model.\n",
    "    - Create a dict (roc_curve_data) with the model names as keys and the values: fpr, tpr, auc.\n",
    "    - Plot the ROC curve for each model with the AUC value in the legend based on roc_curve_data.\n",
    "    - Remove \"Classifier\" from the model names for better readability.\n",
    "    - Save the plot to '{DATA_FOLDER}/roc_curve.png'\n",
    "4. Based on the precision-recall data (pr_curve), create a combined line plot of the precision-recall for each model.\n",
    "    - Create a dict (pr_curve_data) with the model names as keys and the values: precision, recall, auc.\n",
    "    - Plot the Precision-Recall curve for each model with the AUC value in the legend based on pr_curve_data.\n",
    "    - Remove \"Classifier\" from the model names for better readability.\n",
    "    - Save the plot to '{DATA_FOLDER}/pr_curve.png'\n",
    "5. Ouput the links to the plots that were saved.\n",
    "    \"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize AutoGen and iterate thru the dialog with the AI Assistants\n",
    "\n",
    "The AutoGen config requires called 'OAI_CONFIG_LIST' with the following format:\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"model\": \"gpt-4\",\n",
    "        \"api_key\": \"XXXXXXXXXXXXXXXXXXXXXXXX\",\n",
    "        \"base_url\": \"https:/XXXXXXXXXXXX.openai.azure.com/\",\n",
    "        \"api_type\": \"azure\",\n",
    "        \"api_version\": \"2024-02-15-preview\"\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "from autogen.coding import LocalCommandLineCodeExecutor\n",
    "from autogen import Cache\n",
    "\n",
    "# Load the configuration list from the JSON file\n",
    "config_file_or_env = \"OAI_CONFIG_LIST\"\n",
    "config_list = autogen.config_list_from_json(config_file_or_env)\n",
    "\n",
    "# Define the GPT-4 model configuration\n",
    "llm_config = {\n",
    "    \"cache_seed\": 41,\n",
    "    \"temperature\": 0.3,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_tokens\": 4000,\n",
    "    \"config_list\": config_list,\n",
    "    \"timeout\": 600}\n",
    "\n",
    "with Cache.disk(cache_path_root=f\"{CODING_FOLDER}/cache\") as cache:\n",
    "\n",
    "    # create an AssistantAgent named \"Healthcare Specialist\"\n",
    "    healthcare_specialist = autogen.AssistantAgent(\n",
    "        name=\"Healthcare Specialist (AI Assistant)\",\n",
    "        llm_config=llm_config\n",
    "    )\n",
    "\n",
    "    # create an AssistantAgent named \"Data Scientist\"\n",
    "    data_scientist1 = autogen.AssistantAgent(\n",
    "        name=\"Data Scientist (AI Assistant)\",\n",
    "        llm_config=llm_config\n",
    "    )\n",
    "\n",
    "    # create an AssistantAgent named \"Data Scientist\"\n",
    "    data_scientist2 = autogen.AssistantAgent(\n",
    "        name=\"Data Scientist (AI Assistant)\",\n",
    "        llm_config=llm_config\n",
    "    )\n",
    "\n",
    "    # create an AssistantAgent named \"Data Scientist\"\n",
    "    data_scientist3 = autogen.AssistantAgent(\n",
    "        name=\"Data Scientist (AI Assistant)\",\n",
    "        llm_config=llm_config\n",
    "    )\n",
    "\n",
    "    # create an AssistantAgent named \"AI Engineer\"\n",
    "    ai_engineer1 = autogen.AssistantAgent(\n",
    "        name=\"AI Engineer (AI Assistant)\",\n",
    "        llm_config=llm_config\n",
    "    )\n",
    "\n",
    "    # create an AssistantAgent named \"AI Engineer\"\n",
    "    ai_engineer2 = autogen.AssistantAgent(\n",
    "        name=\"AI Engineer (AI Assistant)\",\n",
    "        llm_config=llm_config\n",
    "    )\n",
    "\n",
    "    # create an AssistantAgent named \"Visualizations Expert\"\n",
    "    vis_expert = autogen.AssistantAgent(\n",
    "        name=\"Visualizations Expert (AI Assistant)\",\n",
    "        llm_config=llm_config\n",
    "    )\n",
    "\n",
    "    # create a UserProxyAgent instance named \"User\"\n",
    "    user_proxy = autogen.UserProxyAgent(\n",
    "        name=\"User\",\n",
    "        human_input_mode=\"NEVER\",\n",
    "        max_consecutive_auto_reply=10,\n",
    "        is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "        code_execution_config={\n",
    "            # the executor to run the generated code\n",
    "            \"executor\": LocalCommandLineCodeExecutor(work_dir=CODING_FOLDER, timeout=3600),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    chats = [\n",
    "        {\n",
    "            \"recipient\": healthcare_specialist,\n",
    "            \"message\": tasks[0],\n",
    "            \"clear_history\": True,\n",
    "            \"silent\": False,\n",
    "            \"summary_method\": \"reflection_with_llm\",\n",
    "            \"cache\": cache\n",
    "        },\n",
    "        {\n",
    "            \"recipient\": data_scientist1,\n",
    "            \"message\": tasks[1],\n",
    "            \"summary_method\": None,\n",
    "            \"cache\": cache\n",
    "        },        \n",
    "        {\n",
    "            \"recipient\": data_scientist2,\n",
    "            \"message\": tasks[2],\n",
    "            \"summary_method\": None,\n",
    "            \"cache\": cache\n",
    "        },\n",
    "        {\n",
    "            \"recipient\": data_scientist3,\n",
    "            \"message\": tasks[3],\n",
    "            \"summary_method\": None,\n",
    "            \"cache\": cache\n",
    "        },    \n",
    "        {\n",
    "            \"recipient\": ai_engineer1,\n",
    "            \"message\": tasks[4],\n",
    "            \"summary_method\": \"last_msg\",\n",
    "            \"cache\": cache\n",
    "        },         \n",
    "        {\n",
    "            \"recipient\": ai_engineer2,\n",
    "            \"message\": tasks[5],\n",
    "            \"summary_method\": \"last_msg\",\n",
    "            \"cache\": cache\n",
    "        }, \n",
    "        {\n",
    "            \"recipient\": vis_expert,\n",
    "            \"message\": tasks[6],\n",
    "            \"summary_method\": None,\n",
    "            \"cache\": cache\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    chat_res = user_proxy.initiate_chats(chats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the dialog to a markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Initialize the dialog\n",
    "dialog = \"\"\n",
    "\n",
    "for idx, chat in enumerate(chat_res):\n",
    "    for msg in chat.chat_history:\n",
    "        assistant = chats[idx]['recipient'].name\n",
    "\n",
    "        # Add to/from to dialog\n",
    "        if msg['role'] == \"user\":\n",
    "            dialog += f\"## {assistant} $\\\\Rightarrow$ User\\n\"\n",
    "        elif msg['role'] == \"assistant\":\n",
    "            dialog += f\"## User $\\\\Rightarrow$ {assistant}\\n\"\n",
    "\n",
    "        # Check if message content begins with exitcode\n",
    "        if msg['content'].startswith(\"exitcode:\"):\n",
    "            # Split the message content by newline\n",
    "            lines = msg['content'].split(\"\\n\")\n",
    "            \n",
    "            # Get the exit code\n",
    "            exit_code = lines[0].split(\":\")[1].strip().split(' ')[0]\n",
    "\n",
    "            if exit_code == '0':\n",
    "                dialog += f\"> **_SUCCESS:_** The provided code executed successfully.\\n\\n\"\n",
    "            else:\n",
    "                dialog += f\"> **_ERROR:_** There was an error in the code provided**: {lines[0].split(\":\")[1].strip()}\\n\\n\"\n",
    "\n",
    "            if len(lines[1].split(\":\")) > 1:\n",
    "\n",
    "                # Remove 'Code output'\n",
    "                lines[1] = lines[1].split(\":\")[1].strip() \n",
    "\n",
    "                # Get the code output\n",
    "                output = \"\\n\".join(lines[1:]).strip()\n",
    "\n",
    "                # Display the output\n",
    "                dialog += f\"\\n{output}\\n\\n\"\n",
    "        else:\n",
    "            # Remove 'TERMINATE' from the message content\n",
    "            content = msg['content'].replace(\"TERMINATE\", \"\")\n",
    "\n",
    "            # Remove all content past \"Context:\\n\"\n",
    "            if \"Context:\" in content:\n",
    "                content = content.split(\"Context:\")[0]\n",
    "            \n",
    "            dialog += content + \"\\n\\n\"\n",
    "    \n",
    "        dialog += \"---\\n\"\n",
    "\n",
    "# Display the dialog as markdown\n",
    "display(Markdown(dialog))\n",
    "\n",
    "# Save the dialog to a markdown file\n",
    "with open(f\"{CODING_FOLDER}/dialog.md\", \"w\") as text_file:\n",
    "    text_file.write(dialog)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
